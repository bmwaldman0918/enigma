{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\bmwal_sbkb7fk\\enigma\\.venv\\lib\\site-packages (2.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\n",
      "ERROR: No matching distribution found for pickle\n"
     ]
    }
   ],
   "source": [
    "%pip install torch pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from enigma_decoder_scraped import EnigmaDataset, ResNetDecoder, ResidualBlock, decode_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rnnmodel_old.pkl', 'rb') as f:\n",
    "  model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"scraped_data.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    try:\n",
    "      df = pd.DataFrame(json.load(f))\n",
    "    except Exception as e:\n",
    "        print(f\"{e}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = set(\"\".join(str(df['plain'].tolist() + df['encoded'].tolist())))\n",
    "char_encoder = LabelEncoder()\n",
    "char_encoder.fit(list(all_characters))\n",
    "df[\"plain_processed\"] = df[\"plain\"].apply(lambda x : char_encoder.transform(list(x)))\n",
    "df[\"encoded_processed\"] = df[\"encoded\"].apply(lambda x : char_encoder.transform(list(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding word: SLURC ZK                 \n",
      "Decoded word: vYubnkd\n",
      "Decoding word: vYubnkd\n",
      "Decoded word: sccelsj\n",
      "Decoding word: HX                       \n",
      "Decoded word: mp\n",
      "Decoding word: mp\n",
      "Decoded word: Yr\n",
      "Decoding word: C                        \n",
      "Decoded word: b\n",
      "Decoding word: b\n",
      "Decoded word: s\n",
      "Decoding word: OGNLD CKMKU LMCY         \n",
      "Decoded word: fkoocikjanoisu\n",
      "Decoding word: fkoocikjanoisu\n",
      "Decoded word: rsfagnalYa]nmk\n",
      "Decoding word: BZVJV HWFWZ              \n",
      "Decoded word: ikn]pqpaoj\n",
      "Decoding word: ikn]pqpaoj\n",
      "Decoded word: alkehllapd\n",
      "Decoding word: VLUDR MHJNC              \n",
      "Decoded word: cYupiioa[s\n",
      "Decoding word: cYupiioa[s\n",
      "Decoded word: vkfannoa]j\n",
      "Decoding word: FXNFR TPB                \n",
      "Decoded word: epoltjqZ\n",
      "Decoding word: epoltjqZ\n",
      "Decoded word: hdfYsank\n",
      "Decoding word: LCNQE FONBH U            \n",
      "Decoded word: uhojbeheeYa\n",
      "Decoding word: uhojbeheeYa\n",
      "Decoded word: uhajrnmhhnv\n",
      "Decoding word: PNOQL FZY                \n",
      "Decoded word: rdvj[g[a\n",
      "Decoding word: rdvj[g[a\n",
      "Decoded word: ihoalkYv\n",
      "Decoding word: HZ                       \n",
      "Decoded word: mr\n",
      "Decoding word: mr\n",
      "Decoded word: uk\n",
      "Decoding word: MOUTR TV                 \n",
      "Decoded word: p]uukYp\n",
      "Decoding word: p]uukYp\n",
      "Decoded word: hhbklYo\n",
      "Decoding word: ECNO                     \n",
      "Decoded word: thoc\n",
      "Decoding word: thoc\n",
      "Decoded word: rhaj\n",
      "Decoding word: AIOYR EHJMH PR           \n",
      "Decoded word: Ybvnbeoae[ob\n",
      "Decoding word: Ybvnbeoae[ob\n",
      "Decoded word: rkalhhenevoj\n",
      "Decoding word: PNOQL FZYK               \n",
      "Decoded word: rdve[gk[d\n",
      "Decoding word: rdve[gk[d\n",
      "Decoded word: iYoedke[o\n",
      "Decoding word: HA                       \n",
      "Decoded word: mn\n",
      "Decoding word: mn\n",
      "Decoded word: ir\n",
      "Decoding word: OPVF                     \n",
      "Decoded word: fang\n",
      "Decoding word: fang\n",
      "Decoded word: vakY\n",
      "Decoding word: LCNQE FOF                \n",
      "Decoded word: uhojbhpg\n",
      "Decoding word: uhojbhpg\n",
      "Decoded word: uhajoaoj\n",
      "Decoding word: CCC                      \n",
      "Decoded word: bh[\n",
      "Decoding word: bh[\n",
      "Decoded word: sav\n",
      "Decoding word: CCXIC ZKU                \n",
      "Decoded word: bhjdnkkj\n",
      "Decoding word: bhjdnkkj\n",
      "Decoded word: svjelZYj\n",
      "Decoding word: TA                       \n",
      "Decoded word: nn\n",
      "Decoding word: nn\n",
      "Decoded word: Yr\n",
      "Decoding word: C                        \n",
      "Decoded word: b\n",
      "Decoding word: b\n",
      "Decoded word: s\n",
      "Decoding word: OGNLD CKMKU LMCY         \n",
      "Decoded word: fkoocikjanoisu\n",
      "Decoding word: fkoocikjanoisu\n",
      "Decoded word: rsfagnalYa]nmk\n",
      "Decoding word: XCGL                     \n",
      "Decoded word: qhco\n",
      "Decoding word: qhco\n",
      "Decoded word: haba\n",
      "Decoding word: CCXMA                    \n",
      "Decoded word: bhYjj\n",
      "Decoding word: bhYjj\n",
      "Decoded word: shcld\n",
      "Decoding word: HAPFE PQRBS EQ           \n",
      "Decoded word: mntgkanbhobj\n",
      "Decoding word: mntgkanbhobj\n",
      "Decoded word: ihbYsananond\n",
      "Decoding word: MOXWN R                  \n",
      "Decoded word: p]Yvdb\n",
      "Decoding word: p]Yvdb\n",
      "Decoded word: lhagor\n",
      "Decoding word: OPC                      \n",
      "Decoded word: f[[\n",
      "Decoding word: f[[\n",
      "Decoded word: rhv\n",
      "Decoding word: AIOFE KU                 \n",
      "Decoded word: Ybvgbnj\n",
      "Decoding word: Ybvgbnj\n",
      "Decoded word: rkajokd\n",
      "Decoding word: EENE                     \n",
      "Decoded word: tsom\n",
      "Decoding word: tsom\n",
      "Decoded word: okaj\n",
      "Decoding word: OI                       \n",
      "Decoded word: fb\n",
      "Decoding word: fb\n",
      "Decoded word: rk\n",
      "Decoding word: OPC                      \n",
      "Decoded word: f[[\n",
      "Decoding word: f[[\n",
      "Decoded word: rhv\n",
      "Decoding word: PLUF                     \n",
      "Decoded word: rYug\n",
      "Decoding word: rYug\n",
      "Decoded word: e]bY\n",
      "Decoding word: VOIKX                    \n",
      "Decoded word: cvi[u\n",
      "Decoding word: cvi[u\n",
      "Decoded word: qlevh\n",
      "Decoding word: ZH                       \n",
      "Decoded word: eZ\n",
      "Decoding word: eZ\n",
      "Decoded word: hr\n",
      "Decoding word: TC                       \n",
      "Decoded word: nh\n",
      "Decoding word: nh\n",
      "Decoded word: gv\n",
      "Decoding word: MIYS                     \n",
      "Decoded word: pbcn\n",
      "Decoding word: pbcn\n",
      "Decoded word: lkbl\n",
      "Decoding word: CAB                      \n",
      "Decoded word: bnZ\n",
      "Decoding word: bnZ\n",
      "Decoded word: snj\n",
      "Decoding word: PINXJ                    \n",
      "Decoded word: rb]Za\n",
      "Decoding word: rb]Za\n",
      "Decoded word: ekbbv\n",
      "Decoding word: CCC                      \n",
      "Decoded word: bh[\n",
      "Decoding word: bh[\n",
      "Decoded word: sav\n",
      "Decoding word: AIONX TLBHC F            \n",
      "Decoded word: YbvcoaYohpg\n",
      "Decoding word: YbvcoaYohpg\n",
      "Decoded word: rkoZhva]ooj\n",
      "Decoding word: SM                       \n",
      "Decoded word: ve\n",
      "Decoding word: ve\n",
      "Decoded word: gc\n",
      "Decoding word: BLOLD EZ                 \n",
      "Decoded word: iYvoums\n",
      "Decoding word: iYvoums\n",
      "Decoded word: ckias[d\n",
      "Decoding word: OPC                      \n",
      "Decoded word: f[[\n",
      "Decoding word: f[[\n",
      "Decoded word: rhv\n",
      "Decoding word: AIOFE KU                 \n",
      "Decoded word: Ybvgbnj\n",
      "Decoding word: Ybvgbnj\n",
      "Decoded word: rkajokd\n",
      "Decoding word: EENE                     \n",
      "Decoded word: tsom\n",
      "Decoding word: tsom\n",
      "Decoded word: okaj\n",
      "Decoding word: HA                       \n",
      "Decoded word: mn\n",
      "Decoding word: mn\n",
      "Decoded word: ir\n",
      "Decoding word: C                        \n",
      "Decoded word: b\n",
      "Decoding word: b\n",
      "Decoded word: s\n",
      "Decoding word: AMEDR                    \n",
      "Decoded word: Yeepb\n",
      "Decoding word: Yeepb\n",
      "Decoded word: qhhtr\n",
      "Decoding word: HZ                       \n",
      "Decoded word: mr\n",
      "Decoding word: mr\n",
      "Decoded word: uk\n",
      "Decoding word: INP                      \n",
      "Decoded word: sdt\n",
      "Decoding word: sdt\n",
      "Decoded word: ehb\n",
      "Decoding word: SLCS                     \n",
      "Decoded word: vY[n\n",
      "Decoding word: vY[n\n",
      "Decoded word: scal\n",
      "Decoding word: MLPJE LJYR               \n",
      "Decoded word: pYq]eoanb\n",
      "Decoding word: pYq]eoanb\n",
      "Decoded word: lckjhoYnd\n",
      "Decoding word: CX                       \n",
      "Decoded word: bp\n",
      "Decoding word: bp\n",
      "Decoded word: sa\n",
      "Decoding word: C                        \n",
      "Decoded word: b\n",
      "Decoding word: b\n",
      "Decoded word: s\n",
      "Decoding word: ACNNJ                    \n",
      "Decoded word: Yhota\n",
      "Decoding word: Yhota\n",
      "Decoded word: ovaav\n",
      "Decoding word: SLHER NR                 \n",
      "Decoded word: vYhmeob\n",
      "Decoding word: vYhmeob\n",
      "Decoded word: scbbhoj\n",
      "Decoding word: EIXFM                    \n",
      "Decoded word: Zbjgi\n",
      "Decoding word: Zbjgi\n",
      "Decoded word: skeYv\n",
      "Decoding word: CAB                      \n",
      "Decoded word: bnZ\n",
      "Decoding word: bnZ\n",
      "Decoded word: snj\n",
      "Decoding word: PLKOD EZF                \n",
      "Decoded word: rYgcuepd\n",
      "Decoding word: rYgcuepd\n",
      "Decoded word: Zckjchnj\n",
      "Decoding word: SLURC ZK                 \n",
      "Decoded word: vYubnkd\n",
      "Decoding word: vYubnkd\n",
      "Decoded word: sccelsj\n",
      "Decoding word: KNP                      \n",
      "Decoded word: ]dt\n",
      "Decoding word: ]dt\n",
      "Decoded word: rhv\n",
      "Decoding word: ACCIX NF                 \n",
      "Decoded word: Yh[dqpg\n",
      "Decoding word: Yh[dqpg\n",
      "Decoded word: oafeiij\n",
      "Decoding word: SM                       \n",
      "Decoded word: ve\n",
      "Decoding word: ve\n",
      "Decoded word: gc\n",
      "Decoding word: APXAJ                    \n",
      "Decoded word: Y[uja\n",
      "Decoding word: Y[uja\n",
      "Decoded word: e]klv\n",
      "Decoding word: LCCNJ NX                 \n",
      "Decoded word: uh[ca[Z\n",
      "Decoding word: uh[ca[Z\n",
      "Decoded word: hh]Zpoj\n",
      "Decoding word: HA                       \n",
      "Decoded word: mn\n",
      "Decoding word: mn\n",
      "Decoded word: ir\n",
      "Decoding word:                          \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (0). Kernel size: (1). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m correct, incorrect \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m plain, encoded \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(), df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoded\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()):\n\u001b[1;32m----> 3\u001b[0m   pred \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_encoder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m   pred2 \u001b[38;5;241m=\u001b[39m decode_word(model, pred, char_encoder)\n\u001b[0;32m      5\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m pred \u001b[38;5;241m==\u001b[39m plain:\n",
      "File \u001b[1;32mc:\\Users\\bmwal_sbkb7fk\\Enigma\\enigma_decoder.py:131\u001b[0m, in \u001b[0;36mdecode_word\u001b[1;34m(model, encoded_word, char_encoder)\u001b[0m\n\u001b[0;32m    129\u001b[0m encoded_ids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(char_encoder\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28mlist\u001b[39m(encoded_word\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))))\n\u001b[0;32m    130\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(encoded_ids[np\u001b[38;5;241m.\u001b[39mnewaxis, :], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m--> 131\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    133\u001b[0m decoded_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(char_encoder\u001b[38;5;241m.\u001b[39minverse_transform(predictions))\n",
      "File \u001b[1;32mc:\\Users\\bmwal_sbkb7fk\\Enigma\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bmwal_sbkb7fk\\Enigma\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\bmwal_sbkb7fk\\Enigma\\enigma_decoder.py:98\u001b[0m, in \u001b[0;36mResNetDecoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     97\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_blocks(embedded)\n\u001b[0;32m    100\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bmwal_sbkb7fk\\Enigma\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bmwal_sbkb7fk\\Enigma\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\bmwal_sbkb7fk\\Enigma\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bmwal_sbkb7fk\\Enigma\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[0;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[1;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (0). Kernel size: (1). Kernel size can't be greater than actual input size"
     ]
    }
   ],
   "source": [
    "correct, incorrect = 0, 0\n",
    "for plain, encoded in zip(df[\"plain\"].tolist(), df[\"encoded\"].to_list()):\n",
    "  pred = decode_word(model, encoded, char_encoder)\n",
    "  pred2 = decode_word(model, pred, char_encoder)\n",
    "  if pred == plain:\n",
    "    correct += 1\n",
    "  else:\n",
    "    incorrect += 1\n",
    "print(correct / (correct + incorrect))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
